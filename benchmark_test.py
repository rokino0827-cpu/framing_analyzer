#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°

ç”¨æ³•ï¼š
    PYTHONPATH="/root/autodl-tmp" python framing_analyzer/benchmark_test.py
"""

import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List

import pandas as pd
import numpy as np

# è®¾ç½®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT.parent))

from framing_analyzer import AnalyzerConfig, create_analyzer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING)  # å‡å°‘æ—¥å¿—å™ªéŸ³
logger = logging.getLogger(__name__)

class BenchmarkTest:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.data_path = PROJECT_ROOT / "data/all-the-news-2-1_2025-window_bias_scored_balanced_500_clean.csv"
        self.results = {}
    
    def load_test_data(self, sample_sizes: List[int]) -> Dict[int, List[Dict]]:
        """åŠ è½½ä¸åŒå¤§å°çš„æµ‹è¯•æ•°æ®é›†"""
        print("ğŸ“ Loading benchmark data...")
        
        if not self.data_path.exists():
            # ä½¿ç”¨å†…ç½®æ•°æ®
            base_article = {
                "id": "benchmark_article",
                "title": "Benchmark Test Article",
                "content": "This is a benchmark test article for performance testing. " * 20
            }
            
            datasets = {}
            for size in sample_sizes:
                datasets[size] = [
                    {**base_article, "id": f"benchmark_{i}"}
                    for i in range(size)
                ]
            return datasets
        
        # ä»CSVåŠ è½½
        df = pd.read_csv(self.data_path, encoding="utf-8")
        df = df[df["content"].notna() & df["title"].notna()]
        
        datasets = {}
        for size in sample_sizes:
            if size <= len(df):
                sample_df = df.head(size)
                articles = []
                for idx, row in sample_df.iterrows():
                    articles.append({
                        "id": f"benchmark_{idx}",
                        "title": str(row["title"]),
                        "content": str(row["content"])
                    })
                datasets[size] = articles
            else:
                print(f"âš ï¸  Requested size {size} > available data {len(df)}")
        
        return datasets
    
    def create_benchmark_configs(self) -> Dict[str, AnalyzerConfig]:
        """åˆ›å»ºä¸åŒçš„åŸºå‡†é…ç½®"""
        configs = {}
        
        # åŸºç¡€é…ç½®
        base_config = AnalyzerConfig()
        base_config.teacher.bias_class_index = 1
        base_config.teacher.model_local_path = str(PROJECT_ROOT / "bias_detector_data")
        base_config.output.generate_plots = False  # å…³é—­å›¾è¡¨ç”Ÿæˆä»¥æé«˜é€Ÿåº¦
        
        # 1. å¿«é€Ÿé…ç½®ï¼ˆå°batchï¼‰
        fast_config = base_config
        fast_config.teacher.batch_size = 8
        fast_config.scoring.evidence_count = 3
        configs["fast"] = fast_config
        
        # 2. æ ‡å‡†é…ç½®
        standard_config = base_config
        standard_config.teacher.batch_size = 16
        standard_config.scoring.evidence_count = 5
        configs["standard"] = standard_config
        
        # 3. é«˜ç²¾åº¦é…ç½®ï¼ˆå¤§batchï¼‰
        precision_config = base_config
        precision_config.teacher.batch_size = 32
        precision_config.scoring.evidence_count = 10
        configs["precision"] = precision_config
        
        # 4. çœç•¥æ£€æµ‹é…ç½®
        omission_config = base_config
        omission_config.omission.enabled = True
        omission_config.teacher.batch_size = 16
        configs["omission"] = omission_config
        
        return configs
    
    def benchmark_config(self, config_name: str, config: AnalyzerConfig, 
                        datasets: Dict[int, List[Dict]]) -> Dict:
        """å¯¹å•ä¸ªé…ç½®è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ§ª Benchmarking config: {config_name}")
        
        config_results = {
            'config_name': config_name,
            'batch_size': config.teacher.batch_size,
            'omission_enabled': config.omission.enabled,
            'results': {}
        }
        
        for size, articles in datasets.items():
            print(f"  ğŸ“Š Testing {size} articles...")
            
            try:
                # åˆ›å»ºåˆ†æå™¨
                analyzer = create_analyzer(config)
                
                # è®¡æ—¶åˆ†æ
                start_time = time.time()
                results = analyzer.analyze_batch(articles)
                end_time = time.time()
                
                analysis_time = end_time - start_time
                
                # ç»Ÿè®¡ç»“æœ
                framing_scores = [r.framing_score for r in results['results']]
                
                config_results['results'][size] = {
                    'total_time': analysis_time,
                    'time_per_article': analysis_time / size,
                    'articles_per_second': size / analysis_time,
                    'success_count': len(results['results']),
                    'avg_framing_score': np.mean(framing_scores),
                    'score_std': np.std(framing_scores)
                }
                
                print(f"    âœ… {size} articles in {analysis_time:.2f}s ({size/analysis_time:.1f} articles/s)")
                
            except Exception as e:
                print(f"    âŒ Failed: {e}")
                config_results['results'][size] = {
                    'error': str(e)
                }
        
        return config_results
    
    def run_benchmark(self):
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ Starting performance benchmark...")
        
        # æµ‹è¯•æ•°æ®å¤§å°
        sample_sizes = [1, 5, 10, 20, 50]
        
        # åŠ è½½æ•°æ®
        datasets = self.load_test_data(sample_sizes)
        available_sizes = list(datasets.keys())
        print(f"ğŸ“Š Available test sizes: {available_sizes}")
        
        # åˆ›å»ºé…ç½®
        configs = self.create_benchmark_configs()
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_results = {
            'timestamp': time.time(),
            'test_sizes': available_sizes,
            'configs': {}
        }
        
        for config_name, config in configs.items():
            try:
                result = self.benchmark_config(config_name, config, datasets)
                benchmark_results['configs'][config_name] = result
            except Exception as e:
                print(f"âŒ Config {config_name} failed: {e}")
                benchmark_results['configs'][config_name] = {'error': str(e)}
        
        # ä¿å­˜ç»“æœ
        output_dir = PROJECT_ROOT / "results/benchmark"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        results_file = output_dir / "benchmark_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ Results saved to: {results_file}")
        
        # æ‰“å°æ‘˜è¦
        self.print_benchmark_summary(benchmark_results)
        
        return benchmark_results
    
    def print_benchmark_summary(self, results: Dict):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š PERFORMANCE BENCHMARK SUMMARY")
        print("="*60)
        
        # æ‰¾åˆ°æœ€å¤§æµ‹è¯•å¤§å°
        max_size = 0
        for config_data in results['configs'].values():
            if 'results' in config_data:
                sizes = [int(s) for s in config_data['results'].keys() if s != 'error']
                if sizes:
                    max_size = max(max_size, max(sizes))
        
        if max_size == 0:
            print("âŒ No successful benchmark results")
            return
        
        print(f"ğŸ“ˆ Performance at {max_size} articles:")
        print("-" * 40)
        
        performance_data = []
        for config_name, config_data in results['configs'].items():
            if 'results' in config_data and str(max_size) in config_data['results']:
                result = config_data['results'][str(max_size)]
                if 'error' not in result:
                    performance_data.append({
                        'config': config_name,
                        'time': result['total_time'],
                        'speed': result['articles_per_second'],
                        'batch_size': config_data.get('batch_size', 'N/A')
                    })
        
        # æŒ‰é€Ÿåº¦æ’åº
        performance_data.sort(key=lambda x: x['speed'], reverse=True)
        
        for i, data in enumerate(performance_data):
            rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1}."
            print(f"{rank} {data['config']:12} | {data['speed']:6.1f} art/s | {data['time']:6.2f}s | batch={data['batch_size']}")
        
        print("\nğŸ’¡ Recommendations:")
        if performance_data:
            fastest = performance_data[0]
            print(f"   ğŸš€ Fastest: {fastest['config']} config ({fastest['speed']:.1f} articles/s)")
            
            # å¯»æ‰¾å¹³è¡¡ç‚¹
            balanced = None
            for data in performance_data:
                if data['config'] == 'standard':
                    balanced = data
                    break
            
            if balanced:
                print(f"   âš–ï¸  Balanced: {balanced['config']} config ({balanced['speed']:.1f} articles/s)")
        
        print("="*60)

def main():
    benchmark = BenchmarkTest()
    benchmark.run_benchmark()

if __name__ == "__main__":
    main()
