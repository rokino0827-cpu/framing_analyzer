#!/usr/bin/env python3
"""
æ•°æ®é›†å­—æ®µéªŒè¯è„šæœ¬
éªŒè¯æ‰€æœ‰ä»£ç æ˜¯å¦æ­£ç¡®ä½¿ç”¨æ•°æ®é›†å­—æ®µ

ç”¨æ³•ï¼š
    PYTHONPATH="/root/autodl-tmp" python framing_analyzer/verify_dataset_fields.py
"""

import sys
import pandas as pd
from pathlib import Path

# è®¾ç½®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

def verify_dataset_fields():
    """éªŒè¯æ•°æ®é›†å­—æ®µ"""
    
    print("ğŸ” Verifying dataset fields...")
    
    # é¢„æœŸçš„æ•°æ®é›†å­—æ®µ
    expected_fields = [
        'date', 'author', 'title', 'content', 'url', 
        'section', 'publication', 'bias_label', 'bias_probability'
    ]
    
    data_path = Path("data/all-the-news-2-1_2025-window_bias_scored_balanced_500_clean.csv")
    
    if not data_path.exists():
        print(f"âš ï¸  Data file not found: {data_path}")
        print("Using expected fields for verification")
        actual_fields = expected_fields
    else:
        # è¯»å–æ•°æ®é›†å¹¶æ£€æŸ¥å­—æ®µ
        df = pd.read_csv(data_path, nrows=1)  # åªè¯»å–ç¬¬ä¸€è¡Œæ¥æ£€æŸ¥å­—æ®µ
        actual_fields = list(df.columns)
        
        print(f"ğŸ“Š Dataset found with {len(actual_fields)} columns")
        print(f"Actual fields: {actual_fields}")
    
    # éªŒè¯å­—æ®µåŒ¹é…
    missing_fields = set(expected_fields) - set(actual_fields)
    extra_fields = set(actual_fields) - set(expected_fields)
    
    if missing_fields:
        print(f"âŒ Missing expected fields: {missing_fields}")
    
    if extra_fields:
        print(f"â„¹ï¸  Extra fields in dataset: {extra_fields}")
    
    if not missing_fields:
        print("âœ… All expected fields are present")
    
    # æµ‹è¯•ä»£ç ä¸­çš„å­—æ®µè®¿é—®
    print("\nğŸ§ª Testing field access in code...")
    
    # æ¨¡æ‹Ÿæ•°æ®è¡Œ
    test_row = pd.Series({
        'date': '2023-01-01',
        'author': 'Test Author',
        'title': 'Test Title',
        'content': 'Test content for verification',
        'url': 'https://example.com/test',
        'section': 'Test Section',
        'publication': 'Test Publication',
        'bias_label': 0.5,
        'bias_probability': 0.6
    })
    
    test_df = pd.DataFrame([test_row])
    
    # æµ‹è¯•å­—æ®µè®¿é—®æ¨¡å¼
    try:
        # æµ‹è¯•æ­£ç¡®çš„è®¿é—®æ–¹å¼
        article = {
            "id": test_row.get("url") or f"article_test",
            "title": str(test_row["title"]),
            "content": str(test_row["content"]),
            "publication": test_row.get("publication", "unknown"),
            "date": test_row.get("date", "unknown"),
            "author": test_row.get("author", "unknown"),
            "section": test_row.get("section", "unknown"),
        }
        
        # æµ‹è¯•biaså­—æ®µè®¿é—®
        if "bias_label" in test_df.columns and pd.notna(test_row["bias_label"]):
            article["bias_label"] = test_row["bias_label"]
        
        if "bias_probability" in test_df.columns and pd.notna(test_row["bias_probability"]):
            article["bias_probability"] = float(test_row["bias_probability"])
        
        print("âœ… Field access patterns work correctly")
        print(f"   Created article with {len(article)} fields")
        
        # éªŒè¯biaså­—æ®µ
        if 'bias_label' in article and 'bias_probability' in article:
            print("âœ… Bias fields correctly extracted")
        else:
            print("âš ï¸  Some bias fields missing")
        
    except Exception as e:
        print(f"âŒ Field access test failed: {e}")
        return False
    
    return True

def verify_code_patterns():
    """éªŒè¯ä»£ç ä¸­çš„å­—æ®µä½¿ç”¨æ¨¡å¼"""
    
    print("\nğŸ” Verifying code patterns...")
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶ä¸­çš„å­—æ®µä½¿ç”¨
    files_to_check = [
        "framing_analyzer/comprehensive_test.py",
        "framing_analyzer/test_omission_fusion.py", 
        "framing_analyzer/optimize_fusion_weight.py"
    ]
    
    problematic_patterns = [
        "ground_truth_bias",
        "ground_truth_prob", 
        '"bias_label" in row',  # åº”è¯¥æ˜¯ in df.columns
        '"bias_probability" in row'  # åº”è¯¥æ˜¯ in df.columns
    ]
    
    issues_found = []
    
    for file_path in files_to_check:
        path = Path(file_path)
        if path.exists():
            content = path.read_text(encoding='utf-8')
            
            for pattern in problematic_patterns:
                if pattern in content:
                    issues_found.append(f"{file_path}: {pattern}")
    
    if issues_found:
        print("âŒ Found problematic patterns:")
        for issue in issues_found:
            print(f"   {issue}")
        return False
    else:
        print("âœ… No problematic patterns found")
        return True

def main():
    """ä¸»å‡½æ•°"""
    
    print("="*60)
    print("ğŸ” DATASET FIELD VERIFICATION")
    print("="*60)
    
    # éªŒè¯æ•°æ®é›†å­—æ®µ
    fields_ok = verify_dataset_fields()
    
    # éªŒè¯ä»£ç æ¨¡å¼
    patterns_ok = verify_code_patterns()
    
    print("\n" + "="*60)
    print("ğŸ“Š VERIFICATION SUMMARY")
    print("="*60)
    
    if fields_ok and patterns_ok:
        print("ğŸ‰ All verifications passed!")
        print("âœ… Dataset fields are correctly handled")
        print("âœ… Code patterns are correct")
        print("\nğŸ’¡ You can now run the tests with confidence:")
        print("   PYTHONPATH=\"/root/autodl-tmp\" python framing_analyzer/comprehensive_test.py --sample 10 --enable-omission")
    else:
        print("âŒ Some verifications failed")
        if not fields_ok:
            print("   - Dataset field access issues")
        if not patterns_ok:
            print("   - Problematic code patterns found")
    
    print("="*60)

if __name__ == "__main__":
    main()