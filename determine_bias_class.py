#!/usr/bin/env python3
"""
ç¡®å®šbias_detectoræ¨¡å‹çš„æ­£ç¡®bias_class_index
ä½¿ç”¨å¯¹ç…§æ–‡æœ¬æµ‹è¯•å“ªä¸ªç´¢å¼•å¯¹åº”biasç±»
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

def determine_bias_class_index(model_path="bias_detector_data"):
    """
    ä½¿ç”¨å¯¹ç…§æ–‡æœ¬ç¡®å®šbias_class_index
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        
    Returns:
        æ¨èçš„bias_class_index
    """
    print(f"ğŸ” Loading model from: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True).to(device)
    model.eval()
    
    # æµ‹è¯•æ–‡æœ¬ï¼šä¸­æ€§ vs åè§
    test_texts = [
        "This is a factual report about the economic situation.",  # ä¸­æ€§
        "Those corrupt politicians are destroying our country and should be stopped.",  # åè§
        "The meeting was held at 3 PM yesterday.",  # ä¸­æ€§
        "These people are absolutely disgusting and dangerous to society.",  # åè§
    ]
    
    print(f"ğŸ“Š Model info:")
    print(f"   num_labels: {model.config.num_labels}")
    print(f"   id2label: {model.config.id2label}")
    print(f"   label2id: {model.config.label2id}")
    
    # é¢„æµ‹
    inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors="pt").to(device)
    
    with torch.inference_mode():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()
    
    print(f"\nğŸ“ˆ Prediction results:")
    labels = ["neutral", "biased", "neutral", "biased"]
    
    for i, (text, label) in enumerate(zip(test_texts, labels)):
        print(f"   {i+1}. [{label:7}] {text[:50]}...")
        print(f"      Probs: [0]={probs[i][0]:.3f}, [1]={probs[i][1]:.3f}")
    
    # åˆ†æå“ªä¸ªç´¢å¼•æ›´åƒbiasç±»
    neutral_probs = probs[[0, 2]]  # ä¸­æ€§æ–‡æœ¬çš„æ¦‚ç‡
    biased_probs = probs[[1, 3]]   # åè§æ–‡æœ¬çš„æ¦‚ç‡
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«åœ¨åè§æ–‡æœ¬ä¸Šçš„å¹³å‡æ¦‚ç‡æå‡
    neutral_avg = neutral_probs.mean(axis=0)
    biased_avg = biased_probs.mean(axis=0)
    delta = biased_avg - neutral_avg
    
    print(f"\nğŸ“Š Analysis:")
    print(f"   Neutral texts avg: [0]={neutral_avg[0]:.3f}, [1]={neutral_avg[1]:.3f}")
    print(f"   Biased texts avg:  [0]={biased_avg[0]:.3f}, [1]={biased_avg[1]:.3f}")
    print(f"   Delta (biased-neutral): [0]={delta[0]:+.3f}, [1]={delta[1]:+.3f}")
    
    # ç¡®å®šbias_class_index
    recommended_index = int(np.argmax(delta))
    confidence = abs(delta[recommended_index])
    
    print(f"\nğŸ’¡ Recommendation:")
    print(f"   bias_class_index = {recommended_index}")
    print(f"   Confidence: {confidence:.3f}")
    
    if confidence < 0.1:
        print(f"   âš ï¸  Low confidence - you may want to test with more specific texts")
    else:
        print(f"   âœ… High confidence - index {recommended_index} clearly corresponds to bias class")
    
    # ç”Ÿæˆé…ç½®ä»£ç 
    print(f"\nğŸ”§ Configuration code:")
    print(f"   # Add this to your config:")
    print(f"   config.teacher.bias_class_index = {recommended_index}")
    
    return recommended_index

if __name__ == "__main__":
    try:
        recommended_index = determine_bias_class_index()
        print(f"\nâœ… Recommended bias_class_index: {recommended_index}")
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("Make sure the model is properly loaded and accessible.")