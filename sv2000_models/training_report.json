{
  "data_statistics": {
    "total_samples": 7797,
    "columns": [
      "article_id",
      "title",
      "content",
      "annotator_id",
      "sv_conflict_q1_reflects_disagreement",
      "sv_conflict_q2_refers_to_two_sides",
      "sv_conflict_q3_refers_to_winners_losers_optional",
      "sv_conflict_q4_reproach_between_sides",
      "sv_human_q1_human_example_or_face",
      "sv_human_q2_adjectives_personal_vignettes",
      "sv_human_q3_feelings_empathy",
      "sv_human_q4_how_people_affected",
      "sv_human_q5_visual_information_optional",
      "sv_econ_q1_financial_losses_gains",
      "sv_econ_q2_costs_degree_of_expense",
      "sv_econ_q3_economic_consequences_pursue_or_not",
      "sv_moral_q1_moral_message",
      "sv_moral_q2_morality_god_religious_tenets",
      "sv_moral_q3_social_prescriptions",
      "sv_resp_q1_government_ability_solve",
      "sv_resp_q2_individual_group_responsible",
      "sv_resp_q3_government_responsible",
      "sv_resp_q4_solution_proposed",
      "sv_resp_q5_urgent_action_required_optional",
      "sv_notes",
      "sv_average_score",
      "id",
      "y_conflict",
      "y_conflict_disagreement",
      "y_conflict_sides",
      "y_conflict_optional",
      "y_human",
      "y_human_face",
      "y_human_vignettes",
      "y_human_empathy",
      "y_human_affected",
      "y_human_optional",
      "y_econ",
      "y_econ_gains",
      "y_econ_expense",
      "y_econ_not",
      "y_moral",
      "y_moral_message",
      "y_moral_tenets",
      "y_moral_prescriptions",
      "y_resp",
      "y_resp_solve",
      "y_resp_responsible",
      "y_resp_proposed",
      "y_resp_optional",
      "item_1",
      "item_2",
      "item_3",
      "item_4",
      "item_5",
      "item_6",
      "item_7",
      "item_8",
      "item_9",
      "item_10",
      "item_11",
      "item_12",
      "item_13",
      "item_14",
      "item_15",
      "item_16",
      "item_17",
      "item_18",
      "item_19",
      "item_20",
      "sv_frame_avg"
    ],
    "text_statistics": {
      "mean_length": 3610.886238296781,
      "median_length": 2610.0,
      "min_length": 3,
      "max_length": 33082,
      "std_length": 3734.2669895808517
    },
    "frame_statistics": {
      "y_conflict": {
        "mean": 0.1245030139797358,
        "std": 0.22990734720539377,
        "min": 0.0,
        "max": 1.0,
        "median": 0.0
      },
      "y_human": {
        "mean": 0.20697704245222523,
        "std": 0.2949246690284907,
        "min": 0.0,
        "max": 1.0,
        "median": 0.0
      },
      "y_econ": {
        "mean": 0.18250609208670002,
        "std": 0.25620525624083573,
        "min": 0.0,
        "max": 1.0,
        "median": 0.0
      },
      "y_moral": {
        "mean": 0.04065666281903296,
        "std": 0.13762787022302,
        "min": 0.0,
        "max": 1.0,
        "median": 0.0
      },
      "y_resp": {
        "mean": 0.17147620879825573,
        "std": 0.17067468990399257,
        "min": 0.0,
        "max": 1.0,
        "median": 0.2
      }
    },
    "split_statistics": {
      "train_size": 5457,
      "val_size": 1560,
      "test_size": 780
    },
    "grouping": {
      "enabled": false,
      "group_candidates": 0,
      "strategy": "random"
    },
    "item_coverage": {
      "found": 20,
      "total": 20,
      "missing": []
    }
  },
  "training_history": {
    "train_loss": [
      0.6364767143600866,
      0.47127539538152036
    ],
    "val_loss": [
      0.40589457750320435,
      0.3365202844142914
    ],
    "val_correlation": [
      0.39121265622587886,
      0.510109619745096
    ],
    "learning_rate": [
      0.001,
      0.001
    ],
    "best_epoch": 2,
    "best_val_loss": 0.3365202844142914,
    "best_val_correlation": 0.510109619745096
  },
  "final_metrics": {
    "val": {
      "conflict_pearson": 0.6302667635901298,
      "conflict_spearman": 0.5624876263223775,
      "conflict_mae": 0.15698489230603743,
      "conflict_rmse": 0.1939421972701063,
      "human_pearson": 0.6896008435026629,
      "human_spearman": 0.6325571216805596,
      "human_mae": 0.16335213731675863,
      "human_rmse": 0.21629116683411911,
      "econ_pearson": 0.658430326164814,
      "econ_spearman": 0.6708279087558633,
      "econ_mae": 0.15756215083603864,
      "econ_rmse": 0.2003919982693595,
      "moral_pearson": 0.322121078545117,
      "moral_spearman": 0.3017299707917725,
      "moral_mae": 0.08043206683320239,
      "moral_rmse": 0.12716530993798023,
      "resp_pearson": 0.2855070967222005,
      "resp_spearman": 0.20577849838095896,
      "resp_mae": 0.12666876299927632,
      "resp_rmse": 0.16622003625834078,
      "avg_correlation": 0.5171852217049848,
      "avg_mae": 0.13700000205826268,
      "frame_avg_pearson": 0.5116899226631673,
      "frame_avg_mae": 0.07890594887478737,
      "loss": 0.33451220393180847,
      "pred_stats": {
        "sv_conflict_pred": {
          "mean": 0.12984541058540344,
          "std": 0.0699351504445076
        },
        "sv_human_pred": {
          "mean": 0.20790588855743408,
          "std": 0.17109115421772003
        },
        "sv_econ_pred": {
          "mean": 0.18160755932331085,
          "std": 0.11906267702579498
        },
        "sv_moral_pred": {
          "mean": 0.05638913810253143,
          "std": 0.025611864402890205
        },
        "sv_resp_pred": {
          "mean": 0.181208997964859,
          "std": 0.036393970251083374
        },
        "sv_frame_avg_pred": {
          "mean": 0.15139140188694,
          "std": 0.050478264689445496
        }
      },
      "target_stats": {
        "conflict": {
          "mean": 0.12980769230769232,
          "std": 0.23026453893195617
        },
        "human": {
          "mean": 0.20551282051282052,
          "std": 0.29525368247512446
        },
        "econ": {
          "mean": 0.183974358974359,
          "std": 0.2576181485331515
        },
        "moral": {
          "mean": 0.039316239316239315,
          "std": 0.13190944724278525
        },
        "resp": {
          "mean": 0.17064102564102568,
          "std": 0.17256615040503956
        },
        "frame_avg": {
          "mean": 0.14585042735042733,
          "std": 0.23279918193478255
        }
      }
    },
    "test": {
      "conflict_pearson": 0.6332314056373227,
      "conflict_spearman": 0.6008447625103398,
      "conflict_mae": 0.1625675673907002,
      "conflict_rmse": 0.20168889028118314,
      "human_pearson": 0.6723657896458796,
      "human_spearman": 0.630197172772844,
      "human_mae": 0.17095750225397452,
      "human_rmse": 0.22264934353501323,
      "econ_pearson": 0.6567240284964218,
      "econ_spearman": 0.6435020049274625,
      "econ_mae": 0.15507204742927072,
      "econ_rmse": 0.19561171682606626,
      "moral_pearson": 0.3201378166325802,
      "moral_spearman": 0.2871531914434861,
      "moral_mae": 0.07957084408730396,
      "moral_rmse": 0.1247594718051679,
      "resp_pearson": 0.23754396878555872,
      "resp_spearman": 0.14865653781824403,
      "resp_mae": 0.13188387027917764,
      "resp_rmse": 0.1702486321690321,
      "avg_correlation": 0.5040006018395525,
      "avg_mae": 0.14001036628808539,
      "frame_avg_pearson": 0.4697842880586013,
      "frame_avg_mae": 0.08211673219056209,
      "loss": 0.3398277461528778,
      "pred_stats": {
        "sv_conflict_pred": {
          "mean": 0.13612854480743408,
          "std": 0.07483192533254623
        },
        "sv_human_pred": {
          "mean": 0.2157328873872757,
          "std": 0.17647331953048706
        },
        "sv_econ_pred": {
          "mean": 0.18396979570388794,
          "std": 0.12183470278978348
        },
        "sv_moral_pred": {
          "mean": 0.05804774910211563,
          "std": 0.026933763176202774
        },
        "sv_resp_pred": {
          "mean": 0.18453554809093475,
          "std": 0.037474624812603
        },
        "sv_frame_avg_pred": {
          "mean": 0.15568290650844574,
          "std": 0.05089180916547775
        }
      },
      "target_stats": {
        "conflict": {
          "mean": 0.14038461538461539,
          "std": 0.2405332873583533
        },
        "human": {
          "mean": 0.20435897435897438,
          "std": 0.29859753267906447
        },
        "econ": {
          "mean": 0.1786324786324786,
          "std": 0.2526201363789371
        },
        "moral": {
          "mean": 0.03547008547008547,
          "std": 0.12863957728091555
        },
        "resp": {
          "mean": 0.16974358974358972,
          "std": 0.174554189359669
        },
        "frame_avg": {
          "mean": 0.14571794871794871,
          "std": 0.23457765938348032
        }
      }
    }
  },
  "optimized_weights": {
    "sv_frame_avg_pred": 1.0,
    "bias_score": 0.0,
    "omission_score": 0.0,
    "relative_score": 0.0,
    "quote_score": 0.0
  },
  "validation_results": {
    "is_valid": true,
    "issues": [
      "Missing expected user columns: ['author', 'url', 'section', 'publication', 'avg_stratum']"
    ],
    "recommendations": [
      "Excellent dataset size for robust training",
      "Data from single annotator - consider inter-annotator reliability checks"
    ],
    "detected_format": "user_machine_annotated",
    "column_mapping": {
      "content": "content",
      "title": "title",
      "id": "article_id",
      "y_conflict_questions": [
        "sv_conflict_q1_reflects_disagreement",
        "sv_conflict_q2_refers_to_two_sides",
        "sv_conflict_q3_refers_to_winners_losers_optional",
        "sv_conflict_q4_reproach_between_sides"
      ],
      "y_human_questions": [
        "sv_human_q1_human_example_or_face",
        "sv_human_q2_adjectives_personal_vignettes",
        "sv_human_q3_feelings_empathy",
        "sv_human_q4_how_people_affected",
        "sv_human_q5_visual_information_optional"
      ],
      "y_econ_questions": [
        "sv_econ_q1_financial_losses_gains",
        "sv_econ_q2_costs_degree_of_expense",
        "sv_econ_q3_economic_consequences_pursue_or_not"
      ],
      "y_moral_questions": [
        "sv_moral_q1_moral_message",
        "sv_moral_q2_morality_god_religious_tenets",
        "sv_moral_q3_social_prescriptions"
      ],
      "y_resp_questions": [
        "sv_resp_q1_government_ability_solve",
        "sv_resp_q2_individual_group_responsible",
        "sv_resp_q3_government_responsible",
        "sv_resp_q4_solution_proposed",
        "sv_resp_q5_urgent_action_required_optional"
      ]
    },
    "total_samples": 7797,
    "valid_samples": 7797,
    "question_coverage": {
      "conflict": {
        "available": 4,
        "total": 4,
        "coverage": 1.0,
        "missing": []
      },
      "human": {
        "available": 5,
        "total": 5,
        "coverage": 1.0,
        "missing": []
      },
      "econ": {
        "available": 3,
        "total": 3,
        "coverage": 1.0,
        "missing": []
      },
      "moral": {
        "available": 3,
        "total": 3,
        "coverage": 1.0,
        "missing": []
      },
      "resp": {
        "available": 5,
        "total": 5,
        "coverage": 1.0,
        "missing": []
      }
    },
    "frame_avg_stats": {
      "mean": 0.14522380402718993,
      "std": 0.12007125294170377,
      "min": 0.0,
      "max": 0.7766666666666666,
      "valid_count": 7797
    },
    "annotator_stats": {
      "unique_annotators": 1,
      "samples_per_annotator": {
        "LLM_annotator_v1": 7797
      }
    }
  },
  "best_epoch": 2,
  "best_val_correlation": 0.510109619745096,
  "best_val_loss": 0.3365202844142914,
  "best_val_metrics": {
    "conflict_pearson": 0.6165137617905884,
    "conflict_spearman": 0.5624860209083447,
    "conflict_mae": 0.22748690054584772,
    "conflict_rmse": 0.23674211419343022,
    "human_pearson": 0.6730956164763094,
    "human_spearman": 0.6325571221804184,
    "human_mae": 0.23168299050094227,
    "human_rmse": 0.2591656194647259,
    "econ_pearson": 0.6612779186886477,
    "econ_spearman": 0.6708279087558633,
    "econ_mae": 0.2137641712832145,
    "econ_rmse": 0.2425666780036616,
    "moral_pearson": 0.3164826879509083,
    "moral_spearman": 0.30172997126863826,
    "moral_mae": 0.20283113939321445,
    "moral_rmse": 0.2111063093304033,
    "resp_pearson": 0.2831781138190265,
    "resp_spearman": 0.2057792797460965,
    "resp_mae": 0.13168749776215125,
    "resp_rmse": 0.17315905548565863,
    "avg_correlation": 0.510109619745096,
    "avg_mae": 0.20149053989707402,
    "frame_avg_pearson": 0.4999294766642465,
    "frame_avg_mae": 0.1368070380783234,
    "loss": 0.3365202844142914,
    "pred_stats": {
      "sv_conflict_pred": {
        "mean": 0.2572771906852722,
        "std": 0.05871705338358879
      },
      "sv_human_pred": {
        "mean": 0.31636133790016174,
        "std": 0.11388187110424042
      },
      "sv_econ_pred": {
        "mean": 0.2994483709335327,
        "std": 0.08003106713294983
      },
      "sv_moral_pred": {
        "mean": 0.20927894115447998,
        "std": 0.03713523596525192
      },
      "sv_resp_pred": {
        "mean": 0.22002868354320526,
        "std": 0.036464519798755646
      },
      "sv_frame_avg_pred": {
        "mean": 0.2604789137840271,
        "std": 0.04171879589557648
      }
    },
    "target_stats": {
      "conflict": {
        "mean": 0.12980769230769232,
        "std": 0.23026453893195617
      },
      "human": {
        "mean": 0.20551282051282052,
        "std": 0.29525368247512446
      },
      "econ": {
        "mean": 0.183974358974359,
        "std": 0.2576181485331515
      },
      "moral": {
        "mean": 0.039316239316239315,
        "std": 0.13190944724278525
      },
      "resp": {
        "mean": 0.17064102564102568,
        "std": 0.17256615040503956
      },
      "frame_avg": {
        "mean": 0.14585042735042733,
        "std": 0.23279918193478255
      }
    }
  },
  "calibration": {
    "temperature": [
      0.5225991010665894,
      0.47866255044937134,
      0.4871236979961395,
      0.5054324269294739,
      0.8669776320457458
    ],
    "bias": [
      0.044073332101106644,
      0.04621336981654167,
      0.11281749606132507,
      -0.24660931527614594,
      -0.05306565389037132
    ],
    "calibrated_loss": 0.3421620726585388,
    "optimizer": "LBFGS",
    "max_iter": 50
  },
  "calibrated_model_path": "sv2000_models/best_sv2000_model_calibrated.pt",
  "split_strategy": {
    "enabled": false,
    "group_candidates": 0,
    "strategy": "random"
  }
}