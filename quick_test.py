#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - ç”¨äºæ—¥å¸¸å¼€å‘å’Œè°ƒè¯•

ç”¨æ³•ï¼š
    PYTHONPATH="/root/autodl-tmp" python framing_analyzer/quick_test.py
"""

import sys
import time
import logging
from pathlib import Path

import pandas as pd
import numpy as np

# è®¾ç½®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT.parent))

from framing_analyzer import (
    AnalyzerConfig, 
    create_analyzer,
    verify_bias_class_index
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def quick_test():
    """å¿«é€Ÿæµ‹è¯•ä¸»è¦åŠŸèƒ½"""
    
    print("ğŸš€ Starting quick test...")
    
    # 1. éªŒè¯bias_class_index
    print("\n1ï¸âƒ£  Verifying bias class index...")
    try:
        result = verify_bias_class_index()
        if result and 'config_suggestion' in result:
            bias_index = result['config_suggestion']['bias_class_index']
            print(f"âœ… Recommended bias_class_index: {bias_index}")
        else:
            bias_index = 1
            print(f"âš ï¸  Using default bias_class_index: {bias_index}")
    except Exception as e:
        print(f"âš ï¸  Verification failed, using default: {e}")
        bias_index = 1
    
    # 2. åˆ›å»ºé…ç½®
    print("\n2ï¸âƒ£  Creating configuration...")
    config = AnalyzerConfig()
    config.teacher.bias_class_index = bias_index
    config.teacher.model_local_path = str(PROJECT_ROOT / "bias_detector_data")
    config.output.output_dir = str(PROJECT_ROOT / "results/quick_test")
    print("âœ… Configuration created")
    
    # 3. æµ‹è¯•æ•°æ®
    print("\n3ï¸âƒ£  Loading test data...")
    data_path = PROJECT_ROOT / "data/all-the-news-2-1_2025-window_bias_scored_balanced_500_clean.csv"
    
    if data_path.exists():
        df = pd.read_csv(data_path, encoding="utf-8")
        df = df[df["content"].notna() & df["title"].notna()].head(5)
        
        articles = []
        for idx, row in df.iterrows():
            articles.append({
                "id": f"test_{idx}",
                "title": str(row["title"]),
                "content": str(row["content"])
            })
        print(f"âœ… Loaded {len(articles)} test articles")
    else:
        # ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®
        articles = [
            {
                "id": "test_1",
                "title": "Economic Policy Update",
                "content": "The government announced new economic policies yesterday. These measures are expected to impact various sectors of the economy. Officials stated that the implementation will begin next quarter."
            },
            {
                "id": "test_2", 
                "title": "Technology Innovation Report",
                "content": "A new breakthrough in artificial intelligence has been reported by researchers. The technology promises to revolutionize how we process information. Industry experts are optimistic about its potential applications."
            },
            {
                "id": "test_3",
                "title": "Climate Change Discussion",
                "content": "Scientists continue to study the effects of climate change on global weather patterns. Recent data shows significant changes in temperature and precipitation. Policymakers are considering various response strategies."
            }
        ]
        print(f"âœ… Using {len(articles)} built-in test articles")
    
    # 4. åŸºç¡€åˆ†ææµ‹è¯•
    print("\n4ï¸âƒ£  Testing basic analysis...")
    start_time = time.time()
    
    try:
        analyzer = create_analyzer(config)
        results = analyzer.analyze_batch(articles)
        
        analysis_time = time.time() - start_time
        
        print(f"âœ… Analysis completed in {analysis_time:.2f}s")
        print(f"ğŸ“Š Processed {len(results['results'])} articles")
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        framing_scores = [r.framing_score for r in results['results']]
        print(f"ğŸ“ˆ Framing scores: {[f'{s:.3f}' for s in framing_scores]}")
        print(f"ğŸ“Š Average score: {np.mean(framing_scores):.3f}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ç¯‡æ–‡ç« çš„è¯¦ç»†ç»“æœ
        if results['results']:
            first_result = results['results'][0]
            print(f"\nğŸ“„ Sample result (Article: {first_result.article_id}):")
            print(f"   Framing Score: {first_result.framing_score:.3f}")
            print(f"   Bias Intensity: {first_result.bias_intensity}")
            if hasattr(first_result, 'evidence') and first_result.evidence:
                print(f"   Evidence Count: {len(first_result.evidence)}")
        
    except Exception as e:
        print(f"âŒ Analysis failed: {e}")
        return False
    
    # 5. çœç•¥æ£€æµ‹æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    print("\n5ï¸âƒ£  Testing omission detection (optional)...")
    try:
        omission_config = config
        omission_config.omission.enabled = True
        
        omission_analyzer = create_analyzer(omission_config)
        omission_results = omission_analyzer.analyze_batch(articles[:2])  # åªæµ‹è¯•2ç¯‡
        
        omission_count = sum(1 for r in omission_results['results'] 
                           if hasattr(r, 'omission_result') and r.omission_result)
        
        print(f"âœ… Omission detection test completed")
        print(f"ğŸ“Š Articles with omissions: {omission_count}/{len(omission_results['results'])}")
        
    except Exception as e:
        print(f"âš ï¸  Omission detection test failed: {e}")
    
    print("\nğŸ‰ Quick test completed successfully!")
    print("\nğŸ’¡ To run comprehensive test:")
    print("   python framing_analyzer/comprehensive_test.py --sample 20")
    
    return True

if __name__ == "__main__":
    success = quick_test()
    sys.exit(0 if success else 1)
